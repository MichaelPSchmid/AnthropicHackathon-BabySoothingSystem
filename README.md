# Somni - Intelligenter Baby Monitor

Somni ist ein KI-gestÃ¼tzter Baby Monitor, der automatisch Babygeschrei erkennt und darauf mit beruhigenden Worten reagiert. Das System ist speziell fÃ¼r frische Eltern entwickelt, die gerade nachts UnterstÃ¼tzung bei schreienden Babys benÃ¶tigen.

## ğŸ—ï¸ Projekt-Status

**Aktueller Entwicklungsstand:**
- âœ… **Backend**: VollstÃ¤ndig funktionsfÃ¤hig mit Baby-Schrei-Erkennung und KI-Beruhigung
- âœ… **Frontend**: Fertig entwickelt (React/TypeScript mit Loveable)
- ğŸš§ **Integration**: Frontend-Backend-Integration steht noch aus
- âœ… **Interne Kommunikation**: TCP zwischen Detector und Agent implementiert

## ğŸ¯ Funktionen

- **Intelligente Schrei-Erkennung**: Nutzt Google's YAMNet fÃ¼r prÃ¤zise Baby-Schrei-Detektion
- **KI-Beruhigungs-Agent**: Spricht automatisch beruhigende Worte mit natÃ¼rlicher Stimme
- **Echtzeit-Verarbeitung**: Sofortige Reaktion bei erkanntem Babygeschrei
- **BestÃ¤tigungslogik**: Vermeidet Fehlalarme durch mehrstufige Verifikation
- **LiveKit-Integration**: Hochqualitative Echtzeit-Audio-Ãœbertragung


## ğŸš€ Quick Start

### Voraussetzungen

- **Python 3.9+** fÃ¼r Backend
- **Node.js 18+** fÃ¼r Frontend
- **Mikrofon** fÃ¼r Audio-Eingabe
- **Lautsprecher/KopfhÃ¶rer** fÃ¼r Audio-Ausgabe
- **Internet-Verbindung** fÃ¼r KI-Services

### 1. Repository klonen

```bash
git clone <your-repo-url>
cd somni
```

### 2. Backend einrichten

#### API-Keys konfigurieren
```bash
cd backend/agent
```

Erstelle `.env.local` mit:
```bash
# LiveKit Cloud (https://cloud.livekit.io/)
LIVEKIT_API_KEY=your_livekit_api_key
LIVEKIT_API_SECRET=your_livekit_api_secret
LIVEKIT_URL=wss://your-project.livekit.cloud

# KI Services
ANTHROPIC_API_KEY=your_claude_api_key
ELEVEN_API_KEY=your_elevenlabs_api_key
```

#### Baby-Schrei-Detektor starten
```bash
cd backend/detector

# Virtual Environment erstellen
python -m venv venv_detector

# Aktivieren (Windows)
venv_detector\Scripts\activate
# Aktivieren (macOS/Linux)
source venv_detector/bin/activate

# Dependencies installieren
pip install tensorflow tensorflow-hub sounddevice numpy

# Service starten
python baby_cry_detector_service.py --threshold 0.3
```

#### KI-Beruhigungs-Agent starten
```bash
# Neues Terminal
cd backend/agent

# Agent starten (Dev Mode - Audio Ã¼ber Browser)
uv run baby_soothing_agent.py dev

# Dann Ã¶ffne: https://agents-playground.livekit.io/
# Verbinde mit deinen LiveKit Credentials
```

### 3. Frontend einrichten

```bash
cd frontend

# Dependencies installieren
npm install

# Development Server starten
npm run dev
```

Das Frontend ist vollstÃ¤ndig entwickelt. Du kannst es Ã¼ber das [Loveable-Projekt](https://lovable.dev/projects/bc57c729-7974-4523-a111-622721564084) bearbeiten und anpassen.

**âš ï¸ Integration ausstehend**: Das Frontend kann noch nicht mit dem Backend kommunizieren. Eine REST API oder WebSocket-Verbindung muss noch implementiert werden.

## ğŸ”§ Technischer Ãœberblick

### Backend-Architektur

**Baby-Schrei-Detektor (`detector/`)**
- Nutzt Google's YAMNet fÃ¼r Audio-Klassifikation
- Kontinuierliche Mikrofon-Ãœberwachung
- TCP-Server fÃ¼r Event-Kommunikation
- BestÃ¤tigungslogik: 60% Schrei-Erkennungsrate Ã¼ber 5 Sekunden

**KI-Beruhigungs-Agent (`agent/`)**
- LiveKit Voice Agent mit Claude 4 Sonnet
- ElevenLabs Text-to-Speech (natÃ¼rliche Stimme)
- TCP-Client fÃ¼r Detektor-Events
- Cooldown-System verhindert Spam

**Kommunikation**
- TCP-Socket zwischen Detektor und Agent
- JSON-basierte Event-Messages
- Asynchrone Event-Verarbeitung

### Frontend-Architektur

**Technologie-Stack:**
- React mit TypeScript
- Vite Build-System  
- shadcn/ui Komponenten
- Tailwind CSS
- Loveable-basierte Entwicklung

**Aktuelle Features:**
- Moderne, responsive BenutzeroberflÃ¤che
- Baby Monitor Dashboard
- Einstellungen fÃ¼r Systemkonfiguration
- Status-Anzeigen und Logs

**âš ï¸ Integration fehlt noch:**
- REST API fÃ¼r Backend-Kommunikation
- WebSocket fÃ¼r Live-Updates
- Einstellungen-Synchronisation
- Live-Audio-Visualisierung

## ğŸ“Š System-Monitoring

### Detector Logs verstehen
```
ğŸ” MÃ¶gliches Weinen erkannt (Prob: 0.42) - warte 3.0s fÃ¼r BestÃ¤tigung...
ğŸ‘¶ğŸ”Š WEINEN BESTÃ„TIGT! (65.2% over 5.0s, Avg Prob: 0.45)
ğŸ“Š Status: CRYING | Prob: 0.52 | Clients: 1
ğŸ¤« Stille-Timer gestartet (Prob: 0.08) - brauche 8.0s
âœ… BERUHIGUNG BESTÃ„TIGT! (8.2s kontinuierliche Stille)
```

### Agent Logs verstehen
```
ğŸ¼ Baby Soothing Agent gestartet!
ğŸ“¡ Starte TCP Event Listener...
ğŸ”— Verbindung zum Detektor-Service hergestellt
ğŸ‘¶ğŸ”Š Baby schreit! Starte Beruhigung...
ğŸ—£ï¸ Sage: 'Shh, shh... everything is okay, little one.'
â±ï¸ Cooldown gestartet (10.0s)
```

## ğŸ› ï¸ Konfiguration

### Detektor-Einstellungen

```python
# Empfindlichkeit anpassen
python baby_cry_detector_service.py --threshold 0.2    # Empfindlicher
python baby_cry_detector_service.py --threshold 0.4    # Weniger empfindlich

# Custom Host/Port
python baby_cry_detector_service.py --host localhost --port 9999
```

### Agent-Einstellungen

**Beruhigungs-Modi:**
- `dev`: Audio Ã¼ber Browser (https://agents-playground.livekit.io/)
- `console`: Direkte Audio-Ausgabe Ã¼ber System-Lautsprecher

```bash
# Dev Mode (empfohlen)
uv run baby_soothing_agent.py dev

# Console Mode
uv run baby_soothing_agent.py console
```

## ğŸ” Testing

1. **System-Check**: Beide Services laufen und sind verbunden
2. **Baby-Schrei simulieren**: Lautes Weinen/Schreien ins Mikrofon
3. **Beruhigung hÃ¶ren**: Agent sollte beruhigende Worte sprechen

## ğŸ“‹ Roadmap

### Kurzfristig (HÃ¶chste PrioritÃ¤t)
- [ ] **REST API fÃ¼r Frontend-Backend-Integration** 
  - Status-Endpunkte (Detector-Status, Agent-Status)
  - Konfigurations-Endpunkte (Threshold, Einstellungen)
  - Event-History-Endpunkte
- [ ] **WebSocket-Verbindung fÃ¼r Live-Updates**
  - Real-time Schrei-Ereignisse im Frontend anzeigen
  - Live-Probability-Updates
  - Agent-Status-Updates
- [ ] **Frontend-Backend-Integration testen und debuggen**

### Mittelfristig (Nach Integration)
- [ ] Erweiterte LLM-Integration fÃ¼r personalisiertere Beruhigung
- [ ] Mehrsprachige UnterstÃ¼tzung
- [ ] Mobile App (React Native)
- [ ] Smartphone-Benachrichtigungen
- [ ] Eltern-Dashboard mit detaillierten Statistiken

### Langfristig (Vision)
- [ ] Machine Learning fÃ¼r personalisierte Beruhigungsstrategien
- [ ] Integration mit Smart Home Systemen
- [ ] Video-Ãœberwachung mit Avatar-Darstellung
- [ ] Medizinische Datenanalyse und Empfehlungen

## ğŸ› Troubleshooting

### HÃ¤ufige Probleme

**Detector**
- **"Models not found"**: `pip install tensorflow tensorflow-hub`
- **Mikrofon nicht erkannt**: Mikrofon-Berechtigungen prÃ¼fen
- **Keine Schrei-Erkennung**: Threshold auf 0.2 senken

**Agent**
- **Kein Audio im Dev Mode**: Browser mit LiveKit verbinden
- **ElevenLabs Fehler**: API Key und Credits prÃ¼fen
- **TCP Connection refused**: Detector zuerst starten

**Frontend**
- **Build-Fehler**: `npm ci` fÃ¼r saubere Installation  
- **Hot Reload funktioniert nicht**: Server neu starten
- **Frontend lÃ¤dt nicht**: Port 5173 verfÃ¼gbar? (`lsof -i :5173`)

**Integration**
- **Frontend kann Backend nicht erreichen**: REST API noch nicht implementiert
- **Keine Live-Updates**: WebSocket-Verbindung fehlt noch
- **Einstellungen nicht synchron**: Backend-Frontend-Integration ausstehend

## ğŸ¤ Entwicklung

### Backend-Entwicklung
Das Backend ist vollstÃ¤ndig in Python entwickelt und nutzt moderne async/await-Patterns fÃ¼r optimale Performance.

### Frontend-Entwicklung  
Das Frontend ist vollstÃ¤ndig entwickelt und kann sowohl lokal als auch Ã¼ber das [Loveable-Projekt](https://lovable.dev/projects/bc57c729-7974-4523-a111-622721564084) bearbeitet werden.

### Integration (TO-DO)
Die Integration zwischen Frontend und Backend ist der nÃ¤chste kritische Schritt:

**BenÃ¶tigte API-Endpunkte:**
```python
# Beispiel REST API Struktur (noch zu implementieren)
GET /api/status          # System Status (Detector + Agent)
GET /api/events          # Event History  
POST /api/config         # Konfiguration Ã¤ndern
WebSocket /ws/live       # Live Updates
```

**Frontend-Integration Tasks:**
- API Client implementieren
- WebSocket-Verbindung fÃ¼r Live-Updates
- Status-Dashboard mit Backend-Daten verbinden
- Konfigurationsseite mit Backend synchronisieren

### Beitragen
1. Fork das Repository
2. Erstelle einen Feature-Branch
3. Committe deine Ã„nderungen
4. Push zum Branch
5. Ã–ffne einen Pull Request

## ğŸ“„ Lizenz

[Lizenz noch zu definieren]

## ğŸ™ Credits

- **YAMNet**: Google's Audio-Klassifikations-Modell
- **LiveKit**: Echtzeit-Audio/Video-Infrastruktur
- **Anthropic Claude**: Large Language Model
- **ElevenLabs**: Text-to-Speech-Service
- **Loveable**: Frontend-Entwicklungsplattform

---

**Aktueller Entwicklungsstand**: Frontend und Backend sind einzeln vollstÃ¤ndig funktionsfÃ¤hig. Der nÃ¤chste kritische Schritt ist die Implementation der REST API/WebSocket-Integration, damit das Frontend mit dem Backend kommunizieren kann. Bei Fragen oder Problemen, bitte Issues erstellen oder direkt kontaktieren.